{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f28464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a8badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fls in os.walk('./Data/'):\n",
    "    books = fls[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3668c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(books):    \n",
    "    train = ''\n",
    "    test = ''\n",
    "    for i in books:\n",
    "        print(i)\n",
    "        path = './Data/' + i\n",
    "        f = open(path, encoding = \"utf-8\")\n",
    "        data_ = f.read()\n",
    "        f.close()\n",
    "        train = train + '\\n' + data_[:int(len(data_)*0.75)]\n",
    "        test = test + '\\n' + data_[int(len(data_)*0.75):]\n",
    "    f = open('./Data/train_dataset.txt', 'w', encoding=\"utf-8\")\n",
    "    f.write(train)\n",
    "    f = open('./Data/test_dataset.txt', 'w', encoding=\"utf-8\")\n",
    "    f.write(test)\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1618bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†—É—Å–ª–∞–Ω –∏ –õ—é–¥–º–∏–ª–∞.txt\n",
      "–°–∫–∞–∑–∫–∞ –æ –∑–æ–ª–æ—Ç–æ–º –ø–µ—Ç—É—à–∫–µ.txt\n",
      "–°–∫–∞–∑–∫–∞ –æ –º—ë—Ä—Ç–≤–æ–π —Ü–∞—Ä–µ–≤–Ω–µ.txt\n",
      "–°–∫–∞–∑–∫–∞ –æ –ø–æ–ø–µ –∏ —Ä–∞–±–æ—Ç–Ω–∏–∫–µ –µ–≥–æ –ë–∞–ª–¥–µ.txt\n",
      "–°–∫–∞–∑–∫–∞ –æ —Ü–∞—Ä–µ –°–∞–ª—Ç–∞–Ω–µ.txt\n"
     ]
    }
   ],
   "source": [
    "read_data(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ef9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6cde3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3medium_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502177b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Data/train_dataset.txt'\n",
    "test_path = './Data/test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35639488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mike PK\\anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=64)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=64)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "416ae297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3medium_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./gpt2-sv\",push_to_hub = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e224ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-sv\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 100, # Number of update steps between two evaluations.\n",
    "#     save_steps=100, # after # steps model is saved\n",
    "    warmup_steps=50,# number of warmup steps for learning rate scheduler\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7e35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93386c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 46:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.834400</td>\n",
       "      <td>3.253605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.858000</td>\n",
       "      <td>3.253605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.844000</td>\n",
       "      <td>3.253605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 165\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.8400179036458333, metrics={'train_runtime': 2799.6927, 'train_samples_per_second': 0.536, 'train_steps_per_second': 0.134, 'total_flos': 174131380224000.0, 'train_loss': 1.8400179036458333, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc36fc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"–ë–µ–ª–∫–∞ –ø—Ä—ã–≥–∞–ª–∞ —Å–∞–º–∞\"\n",
    "\n",
    "tokens = tokenizer(prefix, return_tensors='pt')\n",
    "tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5824930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–µ–ª–∫–∞ –ø—Ä—ã–≥–∞–ª–∞ —Å–∞–º–∞ –Ω–µ –∑–Ω–∞—è –∫—É–¥–∞.\n",
      "¬´–ö—É–¥–∞ —Ç—ã, –±–µ–ª–æ—á–∫–∞? ‚Äî –º–æ–ª–≤–∏–ª–∞ –µ–π —Ü–∞—Ä–∏—Ü–∞: ‚Äî\n",
      "–ü–æ–ø–ª—è—à–∏-–∫–∞ –ø–æ–¥ –º–æ—é –¥—É–¥–æ—á–∫—É¬ª.\n",
      "–ù–æ –±–µ–ª–∫–∞ –≤ –æ—Ç–≤–µ—Ç:\n",
      "¬´–Ø –ø–µ—Å–µ–Ω–∫—É –ø–æ—é –æ–¥–Ω–∞,\n",
      "–ê –ø–ª—è—Å–∞—Ç—å –º–Ω–µ –≤–µ–ª—è—Ç –≤—Å–µ –≤–æ–∫—Ä—É–≥¬ª.\n",
      "–¶–∞—Ä—å –∫ –Ω–µ–π —Å –ø–æ–∫–ª–æ–Ω–æ–º –ø–æ–¥—Ö–æ–¥–∏—Ç,\n",
      "–ì–æ–≤–æ—Ä–∏—Ç: ¬´–î–æ–±—Ä—ã–π –¥–µ–Ω—å, –¥–µ–≤–∏—Ü–∞!\n",
      "–ß—Ç–æ –∂–µ —Ç—ã —Ç–∏—Ö–∞, –∫–∞–∫ –¥–µ–Ω—å –Ω–µ–Ω–∞—Å—Ç–Ω—ã–π?\n",
      "–û–ø–µ—á–∞–ª–µ–Ω–∞ —á–µ–º-–Ω–∏–±—É–¥—å?¬ª\n"
     ]
    }
   ],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model.generate(\n",
    "    **tokens, \n",
    "    #end_token=end_token_id,\n",
    "    do_sample=False, \n",
    "    max_length=size+100, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.7,\n",
    "    num_beams=6,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(prefix + result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff05c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model\\config.json\n",
      "Model weights saved in ./model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./model\",push_to_hub = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b91b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
